import asyncio
import logging
import signal
import sys
from pathlib import Path
from urllib.parse import urlparse
from src.crawler import DocsCrawler
from src.logger import setup_logger

# ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
logger = setup_logger()

def signal_handler(sig, frame):
    print("\nä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
    sys.exit(0)

# SIGTSTP (Ctrl+Z) ã‚’ãƒãƒ³ãƒ‰ãƒ«ã—ã¦çµ‚äº†ã•ã›ã‚‹
signal.signal(signal.SIGTSTP, signal_handler)

def main():
    try:
        print("=== ğŸ“ Docs2Notebook Crawler è¨­å®š ===")
        print("å„é …ç›®ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼ˆEnterã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰")
        
        # === 1. URLå…¥åŠ› (å¿…é ˆ) ===
        target_url = ""
        while not target_url:
            target_url = input("\n[1/4] å¯¾è±¡URLã‚’å…¥åŠ› (ä¾‹: https://docs.python.org): ").strip()
            if not target_url:
                print("ã‚¨ãƒ©ãƒ¼: URLã¯å¿…é ˆã§ã™ã€‚")

        # === 2. å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Downloads) ===
        default_download_dir = Path.home() / "Downloads"
        user_output_dir = input(f"\n[2/4] å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {default_download_dir}]: ").strip()
        
        if user_output_dir:
            output_dir_str = user_output_dir
        else:
            output_dir_str = str(default_download_dir)

        # === 3. æœ€å¤§ãƒšãƒ¼ã‚¸æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20) ===
        max_pages = 20
        user_max_pages = input(f"\n[3/4] æœ€å¤§ã‚¯ãƒ­ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸æ•° [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {max_pages}]: ").strip()
        if user_max_pages:
            try:
                max_pages = int(user_max_pages)
            except ValueError:
                print(f"è­¦å‘Š: æ•°å€¤ã§ã¯ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤({max_pages})ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

        # === 4. ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5) ===
        concurrency = 5
        user_concurrency = input(f"\n[4/4] ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æœ€å¤§æ•° [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {concurrency}]: ").strip()
        if user_concurrency:
            try:
                concurrency = int(user_concurrency)
            except ValueError:
                print(f"è­¦å‘Š: æ•°å€¤ã§ã¯ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤({concurrency})ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

        # å‡ºåŠ›ãƒ‘ã‚¹ã®æ§‹ç¯‰
        output_dir = Path(output_dir_str).expanduser()
        if not output_dir.exists():
            try:
                output_dir.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return

        # URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        try:
            parsed = urlparse(target_url)
            domain = parsed.netloc.replace('.', '-')
            path = parsed.path.strip('/')
            
            if path:
                # ãƒ‘ã‚¹ãŒã‚ã‚‹å ´åˆã¯ã€ãƒ‰ãƒ¡ã‚¤ãƒ³å_ãƒ‘ã‚¹å.mdï¼ˆã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¯ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›ï¼‰
                path_str = path.replace('/', '_')
                output_filename = f"{domain}_{path_str}.md"
            else:
                # ãƒ‰ãƒ¡ã‚¤ãƒ³åã®ã¿ã®å ´åˆã¯ã€ãƒ‰ãƒ¡ã‚¤ãƒ³å.md
                output_filename = f"{domain}.md"
                
            if not domain:
                output_filename = "merged_docs.md"
        except Exception:
            output_filename = "merged_docs.md"
        output_file_path = output_dir / output_filename
        
        # è¨­å®šç¢ºèªè¡¨ç¤º
        print("\n" + "="*30)
        print(f"ä»¥ä¸‹ã®è¨­å®šã§å®Ÿè¡Œã—ã¾ã™:")
        print(f"  å¯¾è±¡URL    : {target_url}")
        print(f"  å‡ºåŠ›å…ˆ     : {output_file_path}")
        print(f"  æœ€å¤§ãƒšãƒ¼ã‚¸ : {max_pages}")
        print(f"  ä¸¦åˆ—æ•°     : {concurrency}")
        print("="*30 + "\n")

        # ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–
        crawler = DocsCrawler(
            start_url=target_url, 
            output_file=str(output_file_path), 
            max_concurrent=concurrency,
            max_pages=max_pages
        )
        
        # å®Ÿè¡Œ
        asyncio.run(crawler.run())
        
        print("ğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼")
    except (KeyboardInterrupt, EOFError):
        print("\nä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()