import asyncio
from playwright.async_api import async_playwright
from .logger import setup_logger
from .extractor import ContentExtractor
from .url_manager import UrlManager

logger = setup_logger(__name__)

class DocsCrawler:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹URLã‹ã‚‰é–‹å§‹ã—ã€åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(self, start_url: str, output_file: str, max_concurrent: int = 5, max_pages: int = 20):
        self.output_file = output_file
        self.max_concurrent = max_concurrent
        
        # URLç®¡ç†ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã®å§”è­²
        self.url_manager = UrlManager(start_url, max_pages)
        self.extractor = ContentExtractor()
        
        self.queue = asyncio.Queue()
        # çµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã›ãšã€ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ãŸã‚ã®ãƒ­ãƒƒã‚¯
        self.file_lock = asyncio.Lock()

    async def crawl_page(self, context, url):
        """
        å˜ä¸€ã®ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¦æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚
        """
        if not self.url_manager.can_crawl(url):
            return

        self.url_manager.mark_visited(url)
        logger.info(f"ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {url}")
        
        page = await context.new_page()
        try:
            # ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«ãªã‚‹ã¾ã§å¾…æ©Ÿï¼ˆSPAå¯¾å¿œï¼‰
            await page.goto(url, wait_until="networkidle", timeout=30000)
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡º
            content = await page.content()
            markdown = self.extractor.extract(content, url)
            
            # çµæœã‚’å³åº§ã«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
            await self._save_page_content(markdown)
            
            # ãƒªãƒ³ã‚¯ã®æ¢ç´¢
            hrefs = await page.evaluate('''() => {
                return Array.from(document.querySelectorAll('a[href]')).map(a => a.href);
            }''')
            
            for href in hrefs:
                if self.url_manager.add_discovered_url(href):
                    # æ­£è¦åŒ–ã•ã‚ŒãŸURLãŒè¿”ã•ã‚Œã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã§ã€add_discovered_urlå†…ã§æ­£è¦åŒ–ã—ã¤ã¤
                    # å†åº¦å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€add_discovered_urlã¯boolã‚’è¿”ã™ã®ã¿ã€‚
                    # ã“ã“ã§ã¯ normalize ã—ã¦ã‹ã‚‰ queue ã«å…¥ã‚Œã‚‹ã€‚
                    normalized = self.url_manager.normalize_url(href)
                    if normalized not in self.url_manager.visited:
                        await self.queue.put(normalized)
                    
        except Exception as e:
            logger.error(f"Error crawling {url}: {e}")
        finally:
            await page.close()

    async def _save_page_content(self, content: str):
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«ãƒ•ã‚¡ã‚¤ãƒ«ã¸è¿½è¨˜ã—ã¾ã™ã€‚
        """
        async with self.file_lock:
            with open(self.output_file, 'a', encoding='utf-8') as f:
                f.write(content)

    async def run(self):
        """
        ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ã€‚
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            
            # ã‚­ãƒ¥ãƒ¼ã‚’åˆæœŸåŒ–
            self.queue.put_nowait(self.url_manager.start_url)
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆç©ºã«ã™ã‚‹ï¼‰
            with open(self.output_file, 'w', encoding='utf-8') as f:
                pass
            
            # ã‚­ãƒ¥ãƒ¼ã®å‡¦ç†ã‚’é–‹å§‹
            await self.process_queue(context)

            await browser.close()
            
        self._log_summary()

    def _log_summary(self):
        """
        ã‚¯ãƒ­ãƒ¼ãƒ«çµæœã®ã‚µãƒãƒªãƒ¼ã‚’æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤ºã—ã¾ã™ï¼ˆãƒ­ã‚°å½¢å¼ã§ã¯ãªã„ï¼‰ã€‚
        """
        visited = self.url_manager.visited
        discovered = self.url_manager.discovered
        
        crawled_count = len(visited)
        uncrawled = discovered - visited
        uncrawled_count = len(uncrawled)
        
        # çµæœã‚µãƒãƒªãƒ¼ã¯loggingã§ã¯ãªãprintã‚’ä½¿ç”¨ã—ã¦ã€è¦‹ã‚„ã™ãæ•´å½¢è¡¨ç¤ºã™ã‚‹
        print("\n" + "-" * 40)
        print(f"ğŸ“ˆ ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ã‚µãƒãƒªãƒ¼")
        print("-" * 40)
        print(f"æ¢ç´¢ã—ãŸãƒšãƒ¼ã‚¸ç·æ•°: {crawled_count}")
        print("æ¢ç´¢ã—ãŸãƒšãƒ¼ã‚¸ä¸€è¦§:")
        for url in sorted(visited):
            print(f"  - {url}")
            
        print("-" * 40)
        print(f"ç™ºè¦‹ã•ã‚ŒãŸãŒæœªæ¢ç´¢ã®ãƒšãƒ¼ã‚¸ç·æ•°: {uncrawled_count}")
        if uncrawled_count > 0:
            print("ç™ºè¦‹ã•ã‚ŒãŸãŒæœªæ¢ç´¢ã®ãƒšãƒ¼ã‚¸ä¸€è¦§:")
            for url in sorted(uncrawled):
                print(f"  - {url}")
        print("-" * 40)
        print(f"çµæœã¯ {self.output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

    async def process_queue(self, context):
        """
        éåŒæœŸã®ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ç”¨ã—ã¦ã€ä¸¦è¡Œæ€§ã‚’åˆ¶é™ã—ãªãŒã‚‰ã‚­ãƒ¥ãƒ¼å†…ã®URLã‚’å‡¦ç†ã—ã¾ã™ã€‚
        """
        sem = asyncio.Semaphore(self.max_concurrent)
        
        # å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’è¿½è·¡
        tasks = set()
        
        async def fetch(url):
            async with sem:
                await self.crawl_page(context, url)
        
        # æœ€åˆã®URLã‚’å–å¾—
        if not self.queue.empty():
            first_url = await self.queue.get()
            task = asyncio.create_task(fetch(first_url))
            tasks.add(task)
        
        while tasks:
            # ã„ãšã‚Œã‹ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…æ©Ÿ
            done, pending_tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
            tasks = pending_tasks
            
            for t in done:
                try:
                    await t
                except Exception as e:
                    logger.error(f"ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

            # ã‚­ãƒ¥ãƒ¼ã‚’ç©ºã«ã—ã¦æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
            while not self.queue.empty():
                url = self.queue.get_nowait()
                # ã“ã“ã§ã® visited ãƒã‚§ãƒƒã‚¯ã¯ queue ã«å…¥ã‚Œã‚‹å‰ã«è¡Œã£ã¦ã„ã‚‹ãŒã€å¿µã®ãŸã‚
                if url not in self.url_manager.visited:
                    new_task = asyncio.create_task(fetch(url))
                    tasks.add(new_task)
