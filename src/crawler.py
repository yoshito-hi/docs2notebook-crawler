import asyncio
from playwright.async_api import async_playwright
from urllib.parse import urlparse, urljoin
from .logger import setup_logger
from .extractor import ContentExtractor

logger = setup_logger(__name__)

class DocsCrawler:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹URLã‹ã‚‰é–‹å§‹ã—ã€åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(self, start_url: str, output_file: str, max_concurrent: int = 5, max_pages: int = 20):
        self.start_url = start_url
        self.output_file = output_file
        self.domain = urlparse(start_url).netloc
        self.visited = set()
        self.queue = asyncio.Queue()
        self.extractor = ContentExtractor()

        # çµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã›ãšã€ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ãŸã‚ã®ãƒ­ãƒƒã‚¯
        self.file_lock = asyncio.Lock()
        self.max_concurrent = max_concurrent
        self.max_pages = max_pages
        self.base_path = urlparse(start_url).path if urlparse(start_url).path else "/"
        self.limit_reached_logged = False
        self.discovered = set()
        self.discovered.add(self.start_url)

    def _is_valid_url(self, url: str) -> bool:
        """
        URLãŒã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã¤http/httpsï¼‰ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¾ã™ã€‚
        """
        parsed = urlparse(url)
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ä¸€è‡´ã‚’ç¢ºèª
        if parsed.netloc != self.domain:
            return False
        # http/httpsã‚¹ã‚­ãƒ¼ãƒ ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
        if parsed.scheme not in ('http', 'https'):
            return False
            
        # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹å†…ã«ã‚ã‚‹ã‹ç¢ºèª (ä¾‹: /docs/ ã§é–‹å§‹ã—ãŸå ´åˆã¯ /docs/ é…ä¸‹ã®ã¿)
        if not parsed.path.startswith(self.base_path):
            return False
            
        return True

    def _normalize_url(self, url: str) -> str:
        """
        URLã‹ã‚‰ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»ã—ã¦æ­£è¦åŒ–ã—ã¾ã™ã€‚
        """
        parsed = urlparse(url)
        return parsed._replace(fragment='').geturl()

    async def crawl_page(self, context, url):
        """
        å˜ä¸€ã®ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¦æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚
        """
        if url in self.visited:
            return
            
        # ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if len(self.visited) >= self.max_pages:
            if not self.limit_reached_logged:
                logger.warning(f"æœ€å¤§ã‚¯ãƒ­ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸æ•° {self.max_pages} ã‚’è¶…ãˆã¾ã—ãŸã€‚ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
                self.limit_reached_logged = True
            return

        self.visited.add(url)
        
        logger.info(f"ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {url}")
        
        page = await context.new_page()
        try:
            # ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«ãªã‚‹ã¾ã§å¾…æ©Ÿï¼ˆSPAå¯¾å¿œï¼‰
            await page.goto(url, wait_until="networkidle", timeout=30000)
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡º
            content = await page.content()
            markdown = self.extractor.extract(content, url)
            
            # çµæœã‚’å³åº§ã«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
            await self._save_page_content(markdown)
            
            # ãƒªãƒ³ã‚¯ã®æ¢ç´¢
            hrefs = await page.evaluate('''() => {
                return Array.from(document.querySelectorAll('a[href]')).map(a => a.href);
            }''')
            
            for href in hrefs:
                # æ­£è¦åŒ–ã—ã¦æœ‰åŠ¹æ€§ã‚’ç¢ºèª
                normalized = self._normalize_url(href)
                if self._is_valid_url(normalized):
                    self.discovered.add(normalized)
                    if normalized not in self.visited:
                        await self.queue.put(normalized)
                    
        except Exception as e:
            logger.error(f"Error crawling {url}: {e}")
        finally:
            await page.close()

    async def _save_page_content(self, content: str):
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«ãƒ•ã‚¡ã‚¤ãƒ«ã¸è¿½è¨˜ã—ã¾ã™ã€‚
        """
        async with self.file_lock:
            with open(self.output_file, 'a', encoding='utf-8') as f:
                f.write(content)

    async def run(self):
        """
        ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ã€‚
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            
            # ã‚­ãƒ¥ãƒ¼ã‚’åˆæœŸåŒ–
            self.queue.put_nowait(self._normalize_url(self.start_url))
            
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆç©ºã«ã™ã‚‹ï¼‰
            with open(self.output_file, 'w', encoding='utf-8') as f:
                pass
            
            # ã‚­ãƒ¥ãƒ¼ã®å‡¦ç†ã‚’é–‹å§‹
            await self.process_queue(context)

            await browser.close()
            
        self._log_summary()

    def _log_summary(self):
        """
        ã‚¯ãƒ­ãƒ¼ãƒ«çµæœã®ã‚µãƒãƒªãƒ¼ã‚’æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤ºã—ã¾ã™ï¼ˆãƒ­ã‚°å½¢å¼ã§ã¯ãªã„ï¼‰ã€‚
        """
        crawled_count = len(self.visited)
        uncrawled = self.discovered - self.visited
        uncrawled_count = len(uncrawled)
        
        # çµæœã‚µãƒãƒªãƒ¼ã¯loggingã§ã¯ãªãprintã‚’ä½¿ç”¨ã—ã¦ã€è¦‹ã‚„ã™ãæ•´å½¢è¡¨ç¤ºã™ã‚‹
        print("\n" + "-" * 40)
        print(f"ğŸ“ˆ ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ã‚µãƒãƒªãƒ¼")
        print("-" * 40)
        print(f"æ¢ç´¢ã—ãŸãƒšãƒ¼ã‚¸ç·æ•°: {crawled_count}")
        print("æ¢ç´¢ã—ãŸãƒšãƒ¼ã‚¸ä¸€è¦§:")
        for url in sorted(self.visited):
            print(f"  - {url}")
            
        print("-" * 40)
        print(f"ç™ºè¦‹ã•ã‚ŒãŸãŒæœªæ¢ç´¢ã®ãƒšãƒ¼ã‚¸ç·æ•°: {uncrawled_count}")
        if uncrawled_count > 0:
            print("ç™ºè¦‹ã•ã‚ŒãŸãŒæœªæ¢ç´¢ã®ãƒšãƒ¼ã‚¸ä¸€è¦§:")
            for url in sorted(uncrawled):
                print(f"  - {url}")
        print("-" * 40)
        print(f"çµæœã¯ {self.output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

    async def process_queue(self, context):
        """
        éåŒæœŸã®ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ç”¨ã—ã¦ã€ä¸¦è¡Œæ€§ã‚’åˆ¶é™ã—ãªãŒã‚‰ã‚­ãƒ¥ãƒ¼å†…ã®URLã‚’å‡¦ç†ã—ã¾ã™ã€‚
        """
        sem = asyncio.Semaphore(self.max_concurrent)
        
        # å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’è¿½è·¡
        tasks = set()
        
        async def fetch(url):
            async with sem:
                await self.crawl_page(context, url)
        
        # æœ€åˆã®URLã‚’å–å¾—
        first_url = await self.queue.get()
        
        # æœ€åˆã®ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        task = asyncio.create_task(fetch(first_url))
        tasks.add(task)
        
        while tasks:
            # ã„ãšã‚Œã‹ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…æ©Ÿ
            done, pending_tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
            tasks = pending_tasks
            
            for t in done:
                try:
                    await t
                except Exception as e:
                    logger.error(f"ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

            # ã‚­ãƒ¥ãƒ¼ã‚’ç©ºã«ã—ã¦æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
            while not self.queue.empty():
                url = self.queue.get_nowait()
                if url not in self.visited:
                    new_task = asyncio.create_task(fetch(url))
                    tasks.add(new_task)
