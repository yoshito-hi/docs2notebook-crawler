# URL クローラーアプリの仕様書

**アプリ名：** Docs2Notebook Crawler（ドックス・ツー・ノートブック・クローラー）

## 課題

* 新しい技術（例：Google Antigravity）をキャッチアップする際、公式ドキュメントが複数ページに散らばっており、全容を把握するのに時間がかかる。
* NotebookLM などの AI ツールに学習させたいが、数十〜数百ある URL を手動でコピー＆ペーストしたり、PDF 化して結合するのは非効率的（「苦行」に近い）。
* 既存の Web スクレイピングツールでは、JavaScript で動的に生成されるコンテンツ（SPA）がうまく取得できない場合がある。

## ターゲット

* **効率化を求めるエンジニア:** 新しい技術ドキュメントを「AI と対話しながら」爆速で理解したい人。
* **自律型学習者:** 特定のドキュメントや Wiki を自分専用のナレッジベース（NotebookLM 等）に変換したい人。

## 技術スタック

* **言語:** Python 3.10+
* **ブラウザ操作 (Core):** `Playwright` (Python版)
* *選定理由:* Google Antigravity のような SPA (Single Page Application) に対応するため、実際のブラウザエンジン（Chromium）をヘッドレスで動作させる必要があるため。Selenium よりも高速でモダン。


* **非同期処理:** `asyncio`
* *選定理由:* 複数のページを効率よく（しかしサーバーに負荷をかけすぎず）処理するため。


* **HTML解析 & 加工:** `BeautifulSoup4`
* *選定理由:* 取得した HTML から「ヘッダー」「フッター」「サイドバー」などのノイズを除去し、メインコンテンツ（`<main>` や `<article>`）だけを抽出するため。


* **Markdown変換:** `markdownify`
* *選定理由:* HTML を LLM (NotebookLM) が読みやすい Markdown 形式に変換するため。



## 機能要件 (Core Features)

### 1. インテリジェント・クローリング

* **ルート URL 指定:** ユーザーが指定した開始 URL (例: `.../docs/home`) から探索を開始する。
* **ドメイン制限 (Scope Guard):** 指定されたドメイン（例: `antigravity.google`）外へのリンクは踏まないように制御し、無限クローリングを防ぐ。
* **SPA 対応:** ページ遷移時、JavaScript の実行完了（`networkidle`）を待機してからコンテンツを取得する。

### 2. コンテンツ抽出 & 浄化 (The "AI-Ready" Logic)

* **メインコンテンツ抽出:** ナビゲーションメニューや広告を除外（`nav`, `footer`, `script` タグの削除）し、本文エリアのみを抽出する。
* **Markdown 変換:** 抽出した HTML を Markdown に変換し、リンク構造や見出し構造を保持する。
* **ソース明記:** 各セクションの冒頭に `Source URL: https://...` を自動付与し、NotebookLM で回答生成時に参照元が分かるようにする。

### 3. 出力 (Output)

* **単一ファイル結合:** 取得した全ページの内容を、最終的に**「1つの Markdown ファイル (`merged_docs.md`)」**として出力する。
* *メリット:* NotebookLM へのアップロードが一回で済む。



## 処理フロー概略

1. **初期化:** ルート URL と許可ドメインを設定。
2. **ブラウザ起動:** Playwright でヘッドレスブラウザを立ち上げる。
3. **探索 (Crawl Loop):**
* 未訪問 URL キューから URL を取り出す。
* ページにアクセス & レンダリング待機。
* HTML 取得 & 解析 (BeautifulSoup)。
* ページ内のリンク (`a href`) を抽出し、未訪問かつ同ドメインならキューに追加。
* メインコンテンツを Markdown 化してメモリに保持。


4. **保存:** 全探索終了後、メモリ内のデータを結合してファイルに書き出し。
